{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers accelerate safetensors httpx fastapi uvicorn qwen-vl-utils\n",
        "%pip install -U bitsandbytes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oRihE4jmpIfQ",
        "outputId": "7422d69f-7f17-475f-ec4a-4101598d2d64"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-5.1.0-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (0.7.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (0.28.1)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.128.2)\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.129.0-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.40.0)\n",
            "Collecting qwen-vl-utils\n",
            "  Downloading qwen_vl_utils-0.0.14-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu128)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx) (0.16.0)\n",
            "Requirement already satisfied: starlette<1.0.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.50.0)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (2.12.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.4.2)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.0.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.3.1)\n",
            "Collecting av (from qwen-vl-utils)\n",
            "  Downloading av-16.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from qwen-vl-utils) (11.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from qwen-vl-utils) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.0->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.0->fastapi) (2.41.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->qwen-vl-utils) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->qwen-vl-utils) (2.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Downloading transformers-5.1.0-py3-none-any.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.129.0-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qwen_vl_utils-0.0.14-py3-none-any.whl (8.1 kB)\n",
            "Downloading av-16.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (41.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av, qwen-vl-utils, fastapi, transformers\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.128.2\n",
            "    Uninstalling fastapi-0.128.2:\n",
            "      Successfully uninstalled fastapi-0.128.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 5.0.0\n",
            "    Uninstalling transformers-5.0.0:\n",
            "      Successfully uninstalled transformers-5.0.0\n",
            "Successfully installed av-16.1.0 fastapi-0.129.0 qwen-vl-utils-0.0.14 transformers-5.1.0\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu128)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (26.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.49.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "import time\n",
        "import json\n",
        "import queue\n",
        "import torch\n",
        "import httpx\n",
        "import logging\n",
        "import threading\n",
        "import base64\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Optional, Any, Union, List\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.responses import JSONResponse\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from transformers import (\n",
        "    Qwen2VLForConditionalGeneration,\n",
        "    AutoProcessor\n",
        ")\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "# ------------------------\n",
        "# Logging\n",
        "# ------------------------\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"vlm_extractor\")\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# ------------------------\n",
        "# CONFIG\n",
        "# ------------------------\n",
        "\n",
        "MAX_NEW_TOKENS = 2048\n",
        "MAX_RETRIES = 3\n",
        "WEBHOOK_TIMEOUT = 120\n",
        "\n",
        "# Using Qwen VLM model\n",
        "MODEL_ID = \"Qwen/Qwen2-VL-7B-Instruct\"  # Using Qwen2-VL which is more stable\n",
        "\n",
        "# ------------------------\n",
        "# DATA MODEL\n",
        "# ------------------------\n",
        "\n",
        "@dataclass\n",
        "class ExtractionTask:\n",
        "    task_id: str\n",
        "    prompt: str\n",
        "    system_prompt: str\n",
        "    template: str\n",
        "    webhook_url: str\n",
        "    retries_left: int  # Moved this BEFORE created_at (no default value)\n",
        "    created_at: float  # Now this has a default value\n",
        "    metadata: Dict[str, Any] = None\n",
        "    images: Optional[List[str]] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.metadata is None:\n",
        "            self.metadata = {}\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# VLM ENGINE\n",
        "# ------------------------\n",
        "\n",
        "class VLMProcessor:\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.processor = None\n",
        "        self.lock = threading.Lock()\n",
        "        self.initialized = False\n",
        "\n",
        "    def initialize(self):\n",
        "        if self.initialized:\n",
        "            return\n",
        "\n",
        "        with self.lock:\n",
        "            if self.initialized:\n",
        "                return\n",
        "\n",
        "            logger.info(\"Loading Qwen VLM model...\")\n",
        "\n",
        "            # Load model with appropriate precision\n",
        "            self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "                MODEL_ID,\n",
        "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "                low_cpu_mem_usage=True\n",
        "            )\n",
        "\n",
        "            # Load processor for handling images and text\n",
        "            self.processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "\n",
        "            self.model.eval()\n",
        "            self.initialized = True\n",
        "            logger.info(\"Qwen VLM model loaded successfully\")\n",
        "\n",
        "    def process_image(self, image_input: str) -> Image.Image:\n",
        "        \"\"\"Convert base64 string or URL to PIL Image\"\"\"\n",
        "        if image_input.startswith(('http://', 'https://')):\n",
        "            # Handle URL\n",
        "            import requests\n",
        "            response = requests.get(image_input, stream=True)\n",
        "            response.raise_for_status()\n",
        "            return Image.open(response.raw)\n",
        "        else:\n",
        "            # Handle base64\n",
        "            try:\n",
        "                # Check if it's base64\n",
        "                image_data = base64.b64decode(image_input)\n",
        "                return Image.open(BytesIO(image_data))\n",
        "            except:\n",
        "                # Assume it's a file path\n",
        "                return Image.open(image_input)\n",
        "\n",
        "    def extract(self,\n",
        "                prompt_text: str,\n",
        "                system_prompt: str,\n",
        "                template: str,\n",
        "                images: Optional[List[str]] = None) -> Union[dict, str]:\n",
        "        \"\"\"\n",
        "        Extract structured data from text and images using provided system prompt and template.\n",
        "        Supports both text-only and multimodal inputs.\n",
        "        \"\"\"\n",
        "        self.initialize()\n",
        "\n",
        "        # Format the template with the prompt\n",
        "        formatted_template = template.replace(\"{prompt}\", prompt_text)\n",
        "\n",
        "        # Prepare messages in Qwen's chat format\n",
        "        messages = []\n",
        "\n",
        "        # Prepare user content\n",
        "        user_content = []\n",
        "\n",
        "        # Add images if provided\n",
        "        if images and len(images) > 0:\n",
        "            for img in images:\n",
        "                try:\n",
        "                    # For Qwen2-VL, we need to pass the image as a dict with \"image\" key\n",
        "                    # The processor will handle the conversion\n",
        "                    user_content.append({\n",
        "                        \"type\": \"image\",\n",
        "                        \"image\": img  # Can be URL, path, or PIL Image\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Failed to process image: {e}\")\n",
        "\n",
        "        # Add text prompt with system instruction\n",
        "        text_content = system_prompt + \"\\n\\nOutput JSON:\\n\" + formatted_template\n",
        "        user_content.append({\n",
        "            \"type\": \"text\",\n",
        "            \"text\": text_content\n",
        "        })\n",
        "\n",
        "        messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_content\n",
        "        })\n",
        "\n",
        "        # Apply chat template\n",
        "        text = self.processor.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        # Prepare inputs\n",
        "        image_inputs, video_inputs = process_vision_info(messages)\n",
        "\n",
        "        inputs = self.processor(\n",
        "            text=[text],\n",
        "            images=image_inputs,\n",
        "            videos=video_inputs,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Move to model device if using GPU\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = inputs.to(self.model.device)\n",
        "\n",
        "        # Generate response\n",
        "        with torch.no_grad():\n",
        "            generated_ids = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=MAX_NEW_TOKENS,\n",
        "                temperature=0.0,\n",
        "                do_sample=False,\n",
        "                repetition_penalty=1.1,\n",
        "                eos_token_id=self.processor.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Trim the input part from the output\n",
        "        generated_ids_trimmed = [\n",
        "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "\n",
        "        # Decode the response\n",
        "        raw = self.processor.batch_decode(\n",
        "            generated_ids_trimmed,\n",
        "            skip_special_tokens=True,\n",
        "            clean_up_tokenization_spaces=False\n",
        "        )[0]\n",
        "\n",
        "        result = raw.strip()\n",
        "\n",
        "        # Try to find JSON in the output\n",
        "        json_start = result.find(\"{\")\n",
        "        json_end = result.rfind(\"}\")\n",
        "\n",
        "        if json_start >= 0 and json_end > json_start:\n",
        "            json_str = result[json_start:json_end + 1]\n",
        "\n",
        "            # Repair common JSON issues\n",
        "            json_str = json_str.replace(\"'\", '\"')\n",
        "\n",
        "            # Try to parse the JSON\n",
        "            try:\n",
        "                parsed_json = json.loads(json_str)\n",
        "                logger.info(f\"Successfully parsed JSON from VLM output\")\n",
        "                return parsed_json\n",
        "            except json.JSONDecodeError as e:\n",
        "                logger.warning(f\"Failed to parse JSON from VLM output: {e}\")\n",
        "                logger.warning(f\"Raw output was: {result[:500]}...\")\n",
        "                # Return the raw text if JSON parsing fails\n",
        "                return {\n",
        "                    \"raw_output\": result,\n",
        "                    \"json_error\": str(e),\n",
        "                    \"note\": \"VLM failed to produce valid JSON\"\n",
        "                }\n",
        "        else:\n",
        "            # No JSON found in output\n",
        "            logger.warning(\"No JSON structure found in VLM output\")\n",
        "            return {\n",
        "                \"raw_output\": result,\n",
        "                \"json_error\": \"No JSON structure found in output\",\n",
        "                \"note\": \"VLM failed to produce JSON structure\"\n",
        "            }\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# SINGLE WORKER QUEUE\n",
        "# ------------------------\n",
        "\n",
        "class TaskQueue:\n",
        "    def __init__(self):\n",
        "        self.q = queue.Queue()\n",
        "        self.worker = None\n",
        "        self.stop_event = threading.Event()\n",
        "        self.processor = VLMProcessor()\n",
        "\n",
        "    def start(self):\n",
        "        self.worker = threading.Thread(\n",
        "            target=self.loop,\n",
        "            daemon=True\n",
        "        )\n",
        "        self.worker.start()\n",
        "        logger.info(\"Single worker started\")\n",
        "\n",
        "    def stop(self):\n",
        "        self.stop_event.set()\n",
        "        self.q.put(None)\n",
        "\n",
        "    def enqueue(self, task: ExtractionTask):\n",
        "        self.q.put(task)\n",
        "        logger.info(f\"Queued {task.task_id}\")\n",
        "\n",
        "    def loop(self):\n",
        "        while not self.stop_event.is_set():\n",
        "            task = self.q.get()\n",
        "            if task is None:\n",
        "                break\n",
        "\n",
        "            self.process(task)\n",
        "            self.q.task_done()\n",
        "\n",
        "    def process(self, task: ExtractionTask):\n",
        "        try:\n",
        "            logger.info(f\"Processing {task.task_id}\")\n",
        "\n",
        "            # Extract using provided system prompt, template, and images\n",
        "            extraction_result = self.processor.extract(\n",
        "                prompt_text=task.prompt,\n",
        "                system_prompt=task.system_prompt,\n",
        "                template=task.template,\n",
        "                images=task.images\n",
        "            )\n",
        "\n",
        "            # Always include the result, whether it's valid JSON or raw text\n",
        "            payload = {\n",
        "                \"task_id\": task.task_id,\n",
        "                \"status\": \"completed\",\n",
        "                \"extracted_data\": extraction_result,\n",
        "                \"metadata\": task.metadata\n",
        "            }\n",
        "\n",
        "            self.send_webhook(task.webhook_url, payload)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Task {task.task_id} failed: {e}\")\n",
        "\n",
        "            if task.retries_left > 0:\n",
        "                task.retries_left -= 1\n",
        "                logger.info(f\"Retrying {task.task_id}, {task.retries_left} retries left\")\n",
        "                self.q.put(task)\n",
        "            else:\n",
        "                fail = {\n",
        "                    \"task_id\": task.task_id,\n",
        "                    \"status\": \"failed\",\n",
        "                    \"error\": str(e),\n",
        "                    \"metadata\": task.metadata\n",
        "                }\n",
        "                self.send_webhook(task.webhook_url, fail)\n",
        "\n",
        "        finally:\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    def send_webhook(self, url, payload):\n",
        "        try:\n",
        "            with httpx.Client(timeout=WEBHOOK_TIMEOUT) as client:\n",
        "                response = client.post(url, json=payload)\n",
        "                response.raise_for_status()\n",
        "                logger.info(f\"Webhook sent successfully to {url}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Webhook error: {e}\")\n",
        "\n",
        "\n",
        "task_queue = TaskQueue()\n",
        "\n",
        "# ------------------------\n",
        "# API SCHEMA\n",
        "# ------------------------\n",
        "\n",
        "class ExtractionRequest(BaseModel):\n",
        "    task_id: str\n",
        "    prompt: str\n",
        "    system_prompt: str\n",
        "    template: str\n",
        "    webhook_url: str\n",
        "    images: Optional[List[str]] = None  # List of base64 images, URLs, or file paths\n",
        "    metadata: Optional[Dict[str, Any]] = {}\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# FASTAPI\n",
        "# ------------------------\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "async def startup():\n",
        "    task_queue.start()\n",
        "    # Initialize in background to not block startup\n",
        "    threading.Thread(target=task_queue.processor.initialize, daemon=True).start()\n",
        "\n",
        "@app.on_event(\"shutdown\")\n",
        "async def shutdown():\n",
        "    task_queue.stop()\n",
        "\n",
        "@app.post(\"/extract\")\n",
        "async def extract(req: ExtractionRequest):\n",
        "    try:\n",
        "        task = ExtractionTask(\n",
        "            task_id=req.task_id,\n",
        "            prompt=req.prompt,\n",
        "            system_prompt=req.system_prompt,\n",
        "            template=req.template,\n",
        "            webhook_url=req.webhook_url,\n",
        "            metadata=req.metadata,\n",
        "            images=req.images,\n",
        "            retries_left=MAX_RETRIES,\n",
        "            created_at=time.time()\n",
        "        )\n",
        "\n",
        "        task_queue.enqueue(task)\n",
        "\n",
        "        return {\"status\": \"queued\", \"task_id\": req.task_id}\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error queueing task: {e}\")\n",
        "        raise HTTPException(500, str(e))\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health():\n",
        "    return {\n",
        "        \"status\": \"ok\",\n",
        "        \"queue_size\": task_queue.q.qsize(),\n",
        "        \"model\": MODEL_ID,\n",
        "        \"model_loaded\": task_queue.processor.initialized\n",
        "    }\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfUnqOA-nPtr",
        "outputId": "528d6f92-6534-4db3-ae45-9d54ebc5dd03"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uvicorn app:app --host 0.0.0.0 --port 8000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22Nj0R_OpOFx",
        "outputId": "46ed597a-8307-4b23-d024-ab3cc2e2be5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m1322\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "INFO:vlm_extractor:Single worker started\n",
            "INFO:vlm_extractor:Loading Qwen VLM model...\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-VL-7B-Instruct/eed13092ef92e448dd6875b2a00151bd3f7db0ac/config.json \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: GET https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-VL-7B-Instruct/eed13092ef92e448dd6875b2a00151bd3f7db0ac/config.json \"HTTP/1.1 200 OK\"\n",
            "\rconfig.json: 0.00B [00:00, ?B/s]\rconfig.json: 1.20kB [00:00, 3.40MB/s]\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/adapter_config.json \"HTTP/1.1 404 Not Found\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-VL-7B-Instruct/eed13092ef92e448dd6875b2a00151bd3f7db0ac/config.json \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/model.safetensors \"HTTP/1.1 404 Not Found\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/model.safetensors.index.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-VL-7B-Instruct/eed13092ef92e448dd6875b2a00151bd3f7db0ac/model.safetensors.index.json \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: GET https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-VL-7B-Instruct/eed13092ef92e448dd6875b2a00151bd3f7db0ac/model.safetensors.index.json \"HTTP/1.1 200 OK\"\n",
            "model.safetensors.index.json: 56.5kB [00:00, 103MB/s]\n",
            "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2-VL-7B-Instruct/revision/main \"HTTP/1.1 200 OK\"\n",
            "Downloading (incomplete total...): 0.00B [00:00, ?B/s]\n",
            "Fetching 5 files:   0% 0/5 [00:00<?, ?it/s]\u001b[AINFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/eed13092ef92e448dd6875b2a00151bd3f7db0ac/model-00001-of-00005.safetensors \"HTTP/1.1 302 Found\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/eed13092ef92e448dd6875b2a00151bd3f7db0ac/model-00004-of-00005.safetensors \"HTTP/1.1 302 Found\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/eed13092ef92e448dd6875b2a00151bd3f7db0ac/model-00003-of-00005.safetensors \"HTTP/1.1 302 Found\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/eed13092ef92e448dd6875b2a00151bd3f7db0ac/model-00002-of-00005.safetensors \"HTTP/1.1 302 Found\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/eed13092ef92e448dd6875b2a00151bd3f7db0ac/model-00005-of-00005.safetensors \"HTTP/1.1 302 Found\"\n",
            "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2-VL-7B-Instruct/xet-read-token/eed13092ef92e448dd6875b2a00151bd3f7db0ac \"HTTP/1.1 200 OK\"\n",
            "Downloading (incomplete total...):   0% 0.00/3.86G [00:00<?, ?B/s]INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2-VL-7B-Instruct/xet-read-token/eed13092ef92e448dd6875b2a00151bd3f7db0ac \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2-VL-7B-Instruct/xet-read-token/eed13092ef92e448dd6875b2a00151bd3f7db0ac \"HTTP/1.1 200 OK\"\n",
            "Downloading (incomplete total...):   0% 0.00/11.6G [00:00<?, ?B/s]INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2-VL-7B-Instruct/xet-read-token/eed13092ef92e448dd6875b2a00151bd3f7db0ac \"HTTP/1.1 200 OK\"\n",
            "Downloading (incomplete total...):   0% 0.00/15.5G [00:00<?, ?B/s]INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2-VL-7B-Instruct/xet-read-token/eed13092ef92e448dd6875b2a00151bd3f7db0ac \"HTTP/1.1 200 OK\"\n",
            "Downloading (incomplete total...):  98% 16.3G/16.6G [02:07<00:01, 253MB/s]\n",
            "Downloading (incomplete total...):  99% 16.4G/16.6G [02:08<00:00, 201MB/s]\n",
            "Downloading (incomplete total...): 100% 16.6G/16.6G [02:08<00:00, 277MB/s]\n",
            "Fetching 5 files: 100% 5/5 [02:08<00:00, 25.74s/it]\n",
            "Download complete: 100% 16.6G/16.6G [02:08<00:00, 129MB/s]\n",
            "Loading weights: 100% 730/730 [01:08<00:00, 10.68it/s, Materializing param=model.visual.patch_embed.proj.weight]\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/generation_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-VL-7B-Instruct/eed13092ef92e448dd6875b2a00151bd3f7db0ac/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: GET https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-VL-7B-Instruct/eed13092ef92e448dd6875b2a00151bd3f7db0ac/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "generation_config.json: 100% 244/244 [00:00<00:00, 1.52MB/s]\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/processor_config.json \"HTTP/1.1 404 Not Found\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/preprocessor_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-VL-7B-Instruct/eed13092ef92e448dd6875b2a00151bd3f7db0ac/preprocessor_config.json \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: GET https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-VL-7B-Instruct/eed13092ef92e448dd6875b2a00151bd3f7db0ac/preprocessor_config.json \"HTTP/1.1 200 OK\"\n",
            "preprocessor_config.json: 100% 347/347 [00:00<00:00, 2.19MB/s]\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/processor_config.json \"HTTP/1.1 404 Not Found\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/preprocessor_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-VL-7B-Instruct/eed13092ef92e448dd6875b2a00151bd3f7db0ac/preprocessor_config.json \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2-VL-7B-Instruct/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/processor_config.json \"HTTP/1.1 404 Not Found\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/chat_template.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-VL-7B-Instruct/eed13092ef92e448dd6875b2a00151bd3f7db0ac/chat_template.json \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: GET https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-VL-7B-Instruct/eed13092ef92e448dd6875b2a00151bd3f7db0ac/chat_template.json \"HTTP/1.1 200 OK\"\n",
            "chat_template.json: 1.05kB [00:00, 573kB/s]\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/chat_template.jinja \"HTTP/1.1 404 Not Found\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/audio_tokenizer_config.json \"HTTP/1.1 404 Not Found\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/processor_config.json \"HTTP/1.1 404 Not Found\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/preprocessor_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-VL-7B-Instruct/eed13092ef92e448dd6875b2a00151bd3f7db0ac/preprocessor_config.json \"HTTP/1.1 200 OK\"\n",
            "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/processor_config.json \"HTTP/1.1 404 Not Found\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/preprocessor_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-VL-7B-Instruct/eed13092ef92e448dd6875b2a00151bd3f7db0ac/preprocessor_config.json \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-VL-7B-Instruct/eed13092ef92e448dd6875b2a00151bd3f7db0ac/config.json \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-VL-7B-Instruct/eed13092ef92e448dd6875b2a00151bd3f7db0ac/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: GET https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-VL-7B-Instruct/eed13092ef92e448dd6875b2a00151bd3f7db0ac/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
            "tokenizer_config.json: 4.19kB [00:00, 12.4MB/s]\n",
            "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2-VL-7B-Instruct/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
            "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2-VL-7B-Instruct/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/vocab.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-VL-7B-Instruct/eed13092ef92e448dd6875b2a00151bd3f7db0ac/vocab.json \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: GET https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-VL-7B-Instruct/eed13092ef92e448dd6875b2a00151bd3f7db0ac/vocab.json \"HTTP/1.1 200 OK\"\n",
            "vocab.json: 2.78MB [00:00, 75.8MB/s]\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/merges.txt \"HTTP/1.1 307 Temporary Redirect\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-VL-7B-Instruct/eed13092ef92e448dd6875b2a00151bd3f7db0ac/merges.txt \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: GET https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-VL-7B-Instruct/eed13092ef92e448dd6875b2a00151bd3f7db0ac/merges.txt \"HTTP/1.1 200 OK\"\n",
            "merges.txt: 1.67MB [00:00, 120MB/s]\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/tokenizer.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-VL-7B-Instruct/eed13092ef92e448dd6875b2a00151bd3f7db0ac/tokenizer.json \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: GET https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-VL-7B-Instruct/eed13092ef92e448dd6875b2a00151bd3f7db0ac/tokenizer.json \"HTTP/1.1 200 OK\"\n",
            "tokenizer.json: 7.03MB [00:00, 157MB/s]\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/added_tokens.json \"HTTP/1.1 404 Not Found\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/special_tokens_map.json \"HTTP/1.1 404 Not Found\"\n",
            "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2-VL-7B-Instruct \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/processor_config.json \"HTTP/1.1 404 Not Found\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/video_preprocessor_config.json \"HTTP/1.1 404 Not Found\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/preprocessor_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-VL-7B-Instruct/eed13092ef92e448dd6875b2a00151bd3f7db0ac/preprocessor_config.json \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/processor_config.json \"HTTP/1.1 404 Not Found\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/video_preprocessor_config.json \"HTTP/1.1 404 Not Found\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/resolve/main/preprocessor_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-VL-7B-Instruct/eed13092ef92e448dd6875b2a00151bd3f7db0ac/preprocessor_config.json \"HTTP/1.1 200 OK\"\n",
            "INFO:vlm_extractor:Qwen VLM model loaded successfully\n",
            "\u001b[32mINFO\u001b[0m:     103.59.73.211:0 - \"\u001b[1mGET /docs HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     103.59.73.211:0 - \"\u001b[1mGET /openapi.json HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO:vlm_extractor:Queued url_image_task-1\n",
            "\u001b[32mINFO\u001b[0m:     103.59.73.211:0 - \"\u001b[1mPOST /extract HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO:vlm_extractor:Processing url_image_task-1\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "INFO:vlm_extractor:Successfully parsed JSON from VLM output\n",
            "INFO:httpx:HTTP Request: POST https://c382-103-59-73-211.ngrok-free.app/webhook \"HTTP/1.1 500 Internal Server Error\"\n",
            "ERROR:vlm_extractor:Webhook error: Server error '500 Internal Server Error' for url 'https://c382-103-59-73-211.ngrok-free.app/webhook'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
            "INFO:vlm_extractor:Queued url_image_task-1\n",
            "\u001b[32mINFO\u001b[0m:     103.59.73.211:0 - \"\u001b[1mPOST /extract HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO:vlm_extractor:Processing url_image_task-1\n",
            "INFO:vlm_extractor:Successfully parsed JSON from VLM output\n",
            "INFO:httpx:HTTP Request: POST https://c382-103-59-73-211.ngrok-free.app/webhook \"HTTP/1.1 200 OK\"\n",
            "INFO:vlm_extractor:Webhook sent successfully to https://c382-103-59-73-211.ngrok-free.app/webhook\n",
            "INFO:vlm_extractor:Queued url_image_task-1\n",
            "\u001b[32mINFO\u001b[0m:     103.59.73.211:0 - \"\u001b[1mPOST /extract HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO:vlm_extractor:Processing url_image_task-1\n",
            "INFO:vlm_extractor:Queued url_image_task-2\n",
            "\u001b[32mINFO\u001b[0m:     103.59.73.211:0 - \"\u001b[1mPOST /extract HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO:vlm_extractor:Successfully parsed JSON from VLM output\n",
            "INFO:httpx:HTTP Request: POST https://c382-103-59-73-211.ngrok-free.app/webhook \"HTTP/1.1 200 OK\"\n",
            "INFO:vlm_extractor:Webhook sent successfully to https://c382-103-59-73-211.ngrok-free.app/webhook\n",
            "INFO:vlm_extractor:Processing url_image_task-2\n",
            "INFO:vlm_extractor:Successfully parsed JSON from VLM output\n",
            "INFO:httpx:HTTP Request: POST https://c382-103-59-73-211.ngrok-free.app/webhook \"HTTP/1.1 200 OK\"\n",
            "INFO:vlm_extractor:Webhook sent successfully to https://c382-103-59-73-211.ngrok-free.app/webhook\n",
            "INFO:vlm_extractor:Queued chat_extraction_detailed_1\n",
            "\u001b[32mINFO\u001b[0m:     103.59.73.211:0 - \"\u001b[1mPOST /extract HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO:vlm_extractor:Processing chat_extraction_detailed_1\n",
            "INFO:vlm_extractor:Successfully parsed JSON from VLM output\n",
            "INFO:httpx:HTTP Request: POST https://c382-103-59-73-211.ngrok-free.app/webhook \"HTTP/1.1 200 OK\"\n",
            "INFO:vlm_extractor:Webhook sent successfully to https://c382-103-59-73-211.ngrok-free.app/webhook\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# curl -L -o cloudflared https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 && chmod +x cloudflared && ./cloudflared tunnel --url http://localhost:8000\n"
      ],
      "metadata": {
        "id": "1iqUyHk7rpsA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}